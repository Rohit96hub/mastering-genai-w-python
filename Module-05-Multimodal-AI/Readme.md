# 🎨 Multimodal AI Labs with Gradio & Generative Models
This module showcases how to build AI-powered applications using text, images, audio, and video through Gradio, OpenAI, and multimodal embeddings. The goal is to enable learners to work with various input/output types and build end-to-end AI applications with creative user interfaces.
## 🧪 Included Labs Overview

| Lab | Name                                                          | Modality           | Description                                                                 |
|-----|---------------------------------------------------------------|--------------------|-----------------------------------------------------------------------------|
| 0   | `Lab-0_Text_to_Image_Gen.ipynb`                               | 🖼️ Text → Image     | Generate images from text prompts using OpenAI or Stability AI APIs        |
| 1   | `Lab-1-Gradio_Birthday_Card_Generation.ipynb`                 | 📝 Text            | Build a birthday card generator with themed styles and messages            |
| 2   | `Lab-2-Text-to-Video.ipynb`                                   | 🎥 Text → Video     | Convert story-like prompts into short videos using AI models               |
| 3   | `Lab-3-Image_Understanding.ipynb`                             | 🖼️→🧠 Image → Text   | Use vision models to understand images and generate captions               |
| 4   | `Lab-4-Audio_to_Text.ipynb`                                   | 🎧 Audio → Text     | Transcribe audio files into text using Whisper or similar models           |
| 5   | `Lab-5-Multimodal-Embeddings.ipynb`                           | 🔗 Text + Image     | Compare similarity between image and text using embeddings                 |
| 6   | `Lab-6-Story_and_Export_Video.ipynb`<br>`Lab-6-Story_and_video_Generator.ipynb` | 📜 Multimodal       | Generate story-driven video with narration and AI-generated visuals       |
