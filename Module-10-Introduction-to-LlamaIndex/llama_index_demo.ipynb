{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbQTvJ2MR7j9"
      },
      "source": [
        "# **What is llama-index?**\n",
        "\n",
        "1. LlamaIndex is a framework for building context-augmented generative AI applications with LLMs.\n",
        "\n",
        "https://www.llamaindex.ai/\n",
        "\n",
        "https://llamahub.ai/\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "st5ZxeuzR0Ne"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**OpenAI with LlamaIndex**"
      ],
      "metadata": {
        "id": "uZje_sW9Ciqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-llms-openai"
      ],
      "metadata": {
        "id": "V2SgahX0DKDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "api_key = getpass(\"Enter your OpenAI API key: \")"
      ],
      "metadata": {
        "id": "_i8Twl2XGQrB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3c22ec0-1e24-4aa0-dfb6-0837fbbf291b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "# api_key = input(\"Enter your OpenAI API key: \")\n",
        "llm = OpenAI(api_key=api_key)\n",
        "response = llm.complete(\"Paul Graham is \")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "cyRdOLKICtLB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8fdd09d-d981-4e8d-ab96-090054a49ac3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a computer scientist, entrepreneur, and venture capitalist. He is best known for co-founding the startup accelerator Y Combinator and for his work on programming languages and web development. Graham is also a prolific writer and has published several influential essays on technology, startups, and entrepreneurship.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygC6LY6WnnJZ"
      },
      "source": [
        "# **Huggingface LLMs with llama-index**\n",
        "we'll experiment chat completion using model serving with huggingface API\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gJt2OfipkwiM"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index-llms-huggingface\n",
        "!pip install llama-index-llms-huggingface-api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3HjzoCnrluh3"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "HF_TOKEN=userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PgRa5LYnl3JN"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
        "llm = HuggingFaceInferenceAPI(model_name=\"HuggingFaceH4/zephyr-7b-alpha\", token=HF_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTLkFndQmdTm",
        "outputId": "57b46d8b-5688-4f4b-f55a-5a8d170df0b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ", there was a young woman named Lily. She was a kind and gentle soul, with a heart full of love and compassion. Lily had always been fascinated by the natural world, and she spent most of her free time exploring the forests and fields around her home.\n",
            "\n",
            "One day, as she was wandering through the woods, Lily stumbled upon a small clearing. In the center of the clearing, she saw a beautiful butterfly fluttering its wings. The butterfly was unlike any she had ever seen before. Its wings were a deep shade of blue, and they shimmered in the sunlight.\n",
            "\n",
            "Lily approached the butterfly slowly, not wanting to startle it. As she got closer, she noticed that the butterfly seemed to be in distress. Its wings were torn and ragged, and it was struggling to fly.\n",
            "\n",
            "Lily knew that she had to help the butterfly. She gently picked it up and cradled it in her hands. She could feel the butterfly's heart beating against her palm, and she knew that it was still alive.\n",
            "\n",
            "Lily took the butterfly home with her, and she set about nursing it back to health. She fed it nectar from flowers, and\n"
          ]
        }
      ],
      "source": [
        "completion_response = llm.complete(\"Once upon a time\")\n",
        "print(completion_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhPaqQgvtJ5W"
      },
      "source": [
        "# **Embeddings with huggingface model using llama-index**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "PGbIWnffr9qX"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index-embeddings-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XR8BeSskoztG"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzorAkDvsjlh",
        "outputId": "ba00e545-18a8-4afc-9528-a6ca2684ec33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "768\n",
            "[0.019173700362443924, 0.02873649075627327, -0.012354069389402866, 0.015822136774659157, 0.07908995449542999]\n"
          ]
        }
      ],
      "source": [
        "embeddings = embed_model.get_text_embedding(\"Hello World!\")\n",
        "print(len(embeddings))\n",
        "print(embeddings[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoxstRA5tkPW"
      },
      "source": [
        "# **Directory reader**\n",
        "Easy way to read files from the directory and subdirectory\n",
        "\n",
        "https://docs.llamaindex.ai/en/stable/module_guides/loading/simpledirectoryreader/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "It5r_K-5VyqD"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "reader = SimpleDirectoryReader(input_dir='/content/Data/') # , recursive=True for subdir\n",
        "documents = reader.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(documents)"
      ],
      "metadata": {
        "id": "9V4zjXojVj5T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "befdafbb-4e5d-46fa-b0c9-cf87089bb2fc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "54"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52lTYbeooSNR"
      },
      "source": [
        "# **Chat with own PDF**\n",
        "We'll learn how to create index and store embeddings."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core import Settings"
      ],
      "metadata": {
        "id": "Vd0w-n5ta5_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why Settings?\n",
        "To change the default openAI usage.\n",
        "Follow this Link: https://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/service_context_migration/?h=settings"
      ],
      "metadata": {
        "id": "TQUwieYgAh5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model"
      ],
      "metadata": {
        "id": "R8T2QQRA-_ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformations = [SentenceSplitter(chunk_size=1024)]"
      ],
      "metadata": {
        "id": "z8BdoWSgKUVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5FN_eAcvX1g_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfe7R9Eb8Guu"
      },
      "outputs": [],
      "source": [
        "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model, transformations=transformations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMDxHYGVu_xB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d24c9bf0-e590-4600-84fe-914853cbdb78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index created: <llama_index.core.indices.vector_store.base.VectorStoreIndex object at 0x7982ff505c60>\n"
          ]
        }
      ],
      "source": [
        "print(f\"Index created: {index}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"What are the cloud roles defined in the document?\"\n",
        "query1=\"Who is the author of the pdf?\"\n",
        "query2=\"What is the motive of this document?\""
      ],
      "metadata": {
        "id": "H0dQKLpLIZoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine=index.as_query_engine(similarity_top_k=2)\n",
        "response = query_engine.query(query)"
      ],
      "metadata": {
        "id": "Bj94ujPRXCwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOYnwPBLIoeA",
        "outputId": "39a6afd5-da78-45fb-deb2-ed259f85c3a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "The document defines the following cloud roles:\n",
            "- Cloud Architects\n",
            "- Cloud Engineers\n",
            "- DevOps Engineers\n",
            "\n",
            "These roles are mentioned in the context of efficient implementation and execution of cloud deployment in both public and private clouds. The responsibilities and skill sets for each role are also provided in the document.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Chat with Youtube Video**\n",
        "\n",
        "From llamahub, we can integrate youtube transcript reader and simplify the Data Ingestion.\n",
        "\n",
        "https://llamahub.ai/l/readers/llama-index-readers-youtube-transcript"
      ],
      "metadata": {
        "id": "fyNEQJ1iMCPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-readers-youtube-transcript"
      ],
      "metadata": {
        "id": "vLSz7A9F1wAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Youtube_video = \"https://www.youtube.com/watch?v=4O1rs7mrNDo\"\n",
        "from llama_index.readers.youtube_transcript import YoutubeTranscriptReader\n",
        "loader = YoutubeTranscriptReader()\n",
        "docs = loader.load_data(ytlinks=[\"https://www.youtube.com/watch?v=4O1rs7mrNDo\"])"
      ],
      "metadata": {
        "id": "Lt-gQadwzut2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_youtube = VectorStoreIndex.from_documents(docs, embed_model=embed_model, transformations=transformations)"
      ],
      "metadata": {
        "id": "3hbZ-adM-xuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_youtube=\"What is the motive of this video?\""
      ],
      "metadata": {
        "id": "Fw7DRziE_Dhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine_youtube=index_youtube.as_query_engine(similarity_top_k=2)\n",
        "response_youtube = query_engine_youtube.query(query_youtube)"
      ],
      "metadata": {
        "id": "VfEsjzW-_Qz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response_youtube)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fOgj8Kn_wyK",
        "outputId": "75a35a26-367a-49af-fe34-61a0e5122d7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "The motive of this video is to announce that the creator, Crush Nyak, will be starting a series on Lang chain, which is a Python library for building language processing pipelines using pre-trained models. The video provides an overview of what the series will cover, including practical implementation in the real world industry, and emphasizes the importance of having good knowledge of Python programming language. The creator also mentions that the series will focus on creating end-to-end projects and will cover prerequisites such as understanding the documentation and using open-source tools like DVC, MLflow, and pipelines. The video encourages viewers to support the creator's channel and share the videos with others.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}